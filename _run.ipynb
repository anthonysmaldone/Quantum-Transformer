{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Hybrid Transformer Architecture with a Quantized Self-Attention Mechanism Applied to Molecular Generation.\n",
    "\n",
    "## Summary\n",
    "The success of the self-attention mechanism in classical machine learning models has inspired the development of quantum analogs aimed at reducing computational overhead. Self-attention integrates learnable query and key matrices to calculate attention scores between all pairs of tokens in a sequence that are then multiplied with a learnable value matrix to obtain the output self-attention matrix, enabling the model effectively capture long-range dependencies within the input sequence. Here, we propose a hybrid quantum-classical self-attention mechanism as part of a transformer decoder, the architecture underlying Large Language Models (LLMs). To demonstrate its utility in chemistry, we train this model on the QM9 dataset for conditional generation, providing SMILES strings as input, each labeled with a set of physicochemical properties that serve as conditions during inference. Our theoretical analysis shows that the time complexity of the query-key dot product is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to $\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$  and $d$ represent the sequence length and embedding dimension, respectively. We perform simulations using NVIDIA's CUDA-Q platform, which is designed for efficient GPU scalability. This work provides an avenue for quantum-enhanced Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "**Note**: If running on Google Colab, you must be connected to a GPU runtime to ensure this notebook works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/anthonysmaldone/Quantum-Transformer.git\n",
    "%pip install cudaq==0.9.1\n",
    "%pip install rdkit==2024.9.4\n",
    "%pip install torch==2.5.1+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install pandas==2.2.2\n",
    "%pip install torchdata==0.10.1\n",
    "%pip install tqdm==4.67.1\n",
    "%pip install scikit-learn==1.5.1\n",
    "%pip install seaborn==0.13.2\n",
    "%pip install gdown==5.2.0\n",
    "\n",
    "%cd Quantum-Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Results & Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository provides a script, `reproduce.py`, to facilitate the reproduction of key results from the paper, including model training, figure generation, and inference.\n",
    "\n",
    "The trained checkpoint files for the best and last (20th) epoch for each models are located in `model_checkpoints/`. The figures and inference reproducability functions uses these models to recreate the data from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Generate Figures**\n",
    "To generate figures used in the paper, use the --figures flag. No additional options are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python reproduce.py --figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Run Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform inference on a trained model, use the `--inference-results` flag along with the required `--model` and `--mode options`.\n",
    "\n",
    "Model Options (--model)\n",
    "- quantum – Train the quantum model.\n",
    "- classical_eq – Train the classical equivalent model.\n",
    "- classical – Train the standard classical model.\n",
    "  \n",
    "Training Mode Options (--mode)\n",
    "- sequence – Train using sequence-only data.\n",
    "- conditions – Train using both sequence and condition data.\n",
    "\n",
    "```\n",
    "python reproduce.py --inference-results --model <MODEL_TYPE> --mode <MODE>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "- Run inference on the pre-trained quantum model with SMILES data only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python reproduce.py --inference-results --model classical --mode sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run inference on the pre-trained classical model with SMILES and physicochemical properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python reproduce.py --inference-results --model quantum --mode conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Train Models**\n",
    "To re-train a model, use the `--train-models` flag along with the required `--model` and `--mode` options as explain above.\n",
    "\n",
    "```\n",
    "python reproduce.py --train-models --model <MODEL_TYPE> --mode <MODE>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: These models were trained on Perlmutter using 4 NVIDIA A100 GPUs, and exact numerical reproducability is not garunteed across hardware architectures. Thus, results may slightly differ when running inference and training reproducability functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "A notebook tutorial can be found here (coming soon) and documentation on available functions can be found in `docs/`.\n",
    "\n",
    "We have provided the pretrained model files to be used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model was trained with molecular property embeddings, the `generate_smiles` function allows it to be sampled from conditionally and grants us the option to specify molecular properties. The following properties can be specified:\n",
    "\n",
    "- MW (molecular weight)\n",
    "- HBA (number of hydrogen bond acceptors) \n",
    "- HBD (number of hydrogen bond donors) \n",
    "- nRot (number of rotatable bonds) \n",
    "- nRing (number of rings)\n",
    "- nHet (number of heteroatoms)\n",
    "- TPSA (topological polar surface area)\n",
    "- LogP (octanol-water partition coefficent)\n",
    "- StereoCenters (number of stereocenter)\n",
    "\n",
    "Below is an example of two sampling experiments where molecules are sampled to have a target weight of 120 from the quantum model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.analysis import generate_smiles\n",
    "\n",
    "valid, unique, novel= generate_smiles(\n",
    "                        checkpoint_path=\"./model_checkpoints/quantum_conditions/model_epoch_20.pt\",\n",
    "                        save_dir=\"./generated_molecules/quantum_conditions_MW_120.csv\",\n",
    "                        num_of_model_queries=1000,\n",
    "                        sampling_batch_size=250,\n",
    "                        imputation_dataset_path=\"./dataset/train_dataset.csv\",\n",
    "                        dataset_novelty_check_path=\"./dataset/train_dataset.csv\",\n",
    "                        device=\"gpu\",\n",
    "                        MW=120,\n",
    "                    )\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./generated_molecules/quantum_conditions_MW_120.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_transformer` function trains a transformer model with either classical or quantum attention mechanisms. It supports various hyperparameters to customize training, including learning rate, batch size, number of epochs, and quantum-specific settings such as the number of qubits and quantum gradient calculation.\n",
    "\n",
    "During the training, the parameters for each epoch are saved in the specified checkpoint_dir, along with `most_recent_batch.pt` if the `save_every_n_batches` argument is specified.\n",
    "\n",
    "The below example runs a fully classical model where the number of parameters is equal to the number of parameters used in the quantum model. This is triggered by setting the `classical_parameter_reduction` argument to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.train import train_transformer\n",
    "\n",
    "train_transformer(\n",
    "    training_data=\"./dataset/qm9.csv\",\n",
    "    checkpoint_dir=\"./checkpoints/classical_example/\",\n",
    "    checkpoint_resume_path=None,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.1,\n",
    "    batch_size=256,\n",
    "    epochs=3,\n",
    "    save_every_n_batches=0,\n",
    "    validation_split=0.05,\n",
    "    attn_type=\"classical\",\n",
    "    num_qubits=6,\n",
    "    ansatz_layers=1,\n",
    "    conditional_training=False,\n",
    "    quantum_gradient_method=\"spsa\",\n",
    "    spsa_epsilon=0.01,\n",
    "    sample_percentage=1.0,\n",
    "    seed=42,\n",
    "    classical_parameter_reduction=True,\n",
    "    device=\"gpu\",\n",
    "    qpu_count=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model with molecular property embeddings, `conditional_training` gets set to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.train import train_transformer\n",
    "\n",
    "train_transformer(\n",
    "    training_data=\"./dataset/qm9.csv\",\n",
    "    checkpoint_dir=\"./checkpoints/classical_example_conditions/\",\n",
    "    checkpoint_resume_path=None,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.1,\n",
    "    batch_size=256,\n",
    "    epochs=3,\n",
    "    save_every_n_batches=0,\n",
    "    validation_split=0.05,\n",
    "    attn_type=\"classical\",\n",
    "    num_qubits=6,\n",
    "    ansatz_layers=1,\n",
    "    conditional_training=True,\n",
    "    quantum_gradient_method=\"spsa\",\n",
    "    spsa_epsilon=0.01,\n",
    "    sample_percentage=1.0,\n",
    "    seed=42,\n",
    "    classical_parameter_reduction=True,\n",
    "    device=\"gpu\",\n",
    "    qpu_count=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantum model can be trained by switching the `attn_type` argument to `'quantum'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.train import train_transformer\n",
    "\n",
    "train_transformer(\n",
    "    training_data=\"./dataset/qm9.csv\",\n",
    "    checkpoint_dir=\"./checkpoints/quantum_example/\",\n",
    "    checkpoint_resume_path=None,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.1,\n",
    "    batch_size=256,\n",
    "    epochs=3,\n",
    "    save_every_n_batches=0,\n",
    "    validation_split=0.05,\n",
    "    attn_type=\"quantum\",\n",
    "    num_qubits=6,\n",
    "    ansatz_layers=1,\n",
    "    conditional_training=False,\n",
    "    quantum_gradient_method=\"spsa\",\n",
    "    spsa_epsilon=0.01,\n",
    "    sample_percentage=1.0,\n",
    "    seed=42,\n",
    "    classical_parameter_reduction=True,\n",
    "    device=\"gpu\",\n",
    "    qpu_count=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Other datasets may be specified in the `training_data` argument. However, one has to ensure correct tokenization rules for your task. Currently, these can be modified by `self.smiles_regex` in the `Transformer_Dataset` class, which is located in `src.train`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
